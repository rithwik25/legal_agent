{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Any, TypedDict\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Required libraries\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph components\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the existing classes and functions\n",
    "class Message(TypedDict):\n",
    "    role: str  # \"human\" or \"ai\"\n",
    "    content: str\n",
    "    timestamp: str\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    \"\"\"State for the legal chatbot.\"\"\"\n",
    "    query: str  # Original user query\n",
    "    enhanced_query: str  # Query after processing by supervisor\n",
    "    context: List[str]  # Retrieved legal content\n",
    "    summary: str  # Simplified legal information\n",
    "    answer: str  # Final response to user\n",
    "    legal_references: List[str]  # Sources of information\n",
    "    conversation_history: List[Message]  # Full conversation history\n",
    "    next_agent: str  # To control flow in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching mechanism for repeated queries\n",
    "class QueryCache:\n",
    "    def __init__(self, max_size=100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def get(self, query: str) -> str:\n",
    "        \"\"\"Retrieve cached response for a query.\"\"\"\n",
    "        # Use a hash of the query to create a consistent key\n",
    "        query_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "        return self.cache.get(query_hash)\n",
    "\n",
    "    def set(self, query: str, response: str):\n",
    "        \"\"\"Cache a response for a query.\"\"\"\n",
    "        query_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "        \n",
    "        # If cache is full, remove the oldest entry\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        \n",
    "        self.cache[query_hash] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalChatbot:\n",
    "    def __init__(self, document_path='litigation.txt', vector_store_path='litigation_faiss_index'):\n",
    "        \n",
    "        self.query_cache = QueryCache() # Initialize caching\n",
    "        self.conversation_history = [] # Initialize conversation history\n",
    "        \n",
    "        # Load or create vector store\n",
    "        self.vector_store_path = vector_store_path\n",
    "        self.embeddings = OpenAIEmbeddings(api_key=os.getenv('OPENAI_API_KEY'), model=\"text-embedding-3-small\")\n",
    "        self.vector_store = self._load_or_create_vector_store(document_path)\n",
    "        \n",
    "        # Set up retriever\n",
    "        self.retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "        \n",
    "        # Set up LLM\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\", \n",
    "            temperature=0.2, \n",
    "            streaming=True,  # Enable streaming\n",
    "            api_key=os.getenv('OPENAI_API_KEY')\n",
    "        )\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = self._build_graph().compile()\n",
    "\n",
    "    def _load_or_create_vector_store(self, document_path: str) -> FAISS:\n",
    "        \"\"\"\n",
    "        Load existing FAISS index or create a new one if it doesn't exist.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.vector_store_path):\n",
    "            try:\n",
    "                # Load existing vector store\n",
    "                return FAISS.load_local(self.vector_store_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load existing vector store: {e}\")\n",
    "        \n",
    "        # If no existing store, create a new one\n",
    "        sample_docs = self._load_documents([document_path])\n",
    "        vector_store = self._create_vector_store(sample_docs)\n",
    "        \n",
    "        # Save the new vector store\n",
    "        vector_store.save_local(self.vector_store_path)\n",
    "        \n",
    "        return vector_store\n",
    "\n",
    "    def _load_documents(self, document_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"Load documents from various file formats with hierarchical chunking for txt files.\"\"\"\n",
    "        documents = []\n",
    "        for path in document_paths:\n",
    "            if path.endswith('.txt'):\n",
    "                text_content = self._load_from_text_file(path)\n",
    "                chunked_docs = self._hierarchical_chunking(text_content, path)\n",
    "                documents.extend(chunked_docs)\n",
    "        return documents\n",
    "\n",
    "    def _load_from_text_file(self, file_path: str) -> str:\n",
    "        \"\"\"Load raw text content from a file.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def _hierarchical_chunking(self, text: str, file_path: str = 'unknown') -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split text hierarchically into documents with metadata.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        sections = re.split(r'\\n\\s\\n\\s\\n+', text)\n",
    "\n",
    "        for section_idx, section_content in enumerate(sections):\n",
    "            if not section_content.strip():\n",
    "                continue\n",
    "\n",
    "            section_lines = section_content.strip().split('\\n', 1)\n",
    "            section_title = section_lines[0].strip() if len(section_lines) > 0 else f\"Section {section_idx + 1}\"\n",
    "            section_text = section_lines[1] if len(section_lines) > 1 else section_lines[0]\n",
    "\n",
    "            paragraphs = re.split(r'\\n\\s*\\n+', section_text)\n",
    "\n",
    "            for para_idx, paragraph in enumerate(paragraphs):\n",
    "                if not paragraph.strip():\n",
    "                    continue\n",
    "\n",
    "                para_content = paragraph.strip()\n",
    "                if para_idx > 0:\n",
    "                    para_content = f\"{section_title}\\n\\n{para_content}\"\n",
    "\n",
    "                doc = Document(\n",
    "                    page_content=para_content,\n",
    "                    metadata={\n",
    "                        \"source\": os.path.basename(file_path),\n",
    "                        \"section\": section_title,\n",
    "                        \"section_idx\": section_idx,\n",
    "                        \"paragraph_idx\": para_idx,\n",
    "                        \"section_paragraph\": f\"{section_idx}.{para_idx}\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _create_vector_store(self, documents: List[Document]) -> FAISS:\n",
    "        \"\"\"\n",
    "        Creates a vector store with intelligent text splitting.\n",
    "        \"\"\"\n",
    "        splits = []\n",
    "\n",
    "        for doc in documents:\n",
    "            if len(doc.page_content) > 1500:\n",
    "                text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=1500,\n",
    "                    chunk_overlap=150,\n",
    "                    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "                )\n",
    "                long_splits = text_splitter.split_documents([doc])\n",
    "                splits.extend(long_splits)\n",
    "            else:\n",
    "                splits.append(doc)\n",
    "\n",
    "        return FAISS.from_documents(splits, self.embeddings)\n",
    "\n",
    "    def supervisor_agent(self, state: ChatState) -> ChatState:\n",
    "        \"\"\"\n",
    "        Supervisor agent that:\n",
    "        1. Processes the incoming query with conversation context\n",
    "        2. Determines the next step in the workflow\n",
    "        3. Handles final response generation and memory updates\n",
    "        \"\"\"\n",
    "        # If we're just starting (no enhanced query yet), do preprocessing\n",
    "        if \"enhanced_query\" not in state or not state[\"enhanced_query\"]:\n",
    "            return self._preprocess_query(state)\n",
    "\n",
    "        # If we have a summary but no answer, generate the final response\n",
    "        if state.get(\"summary\") and not state.get(\"answer\"):\n",
    "            return self._generate_response(state)\n",
    "\n",
    "        # Default next step is query processing\n",
    "        return {**state, \"next_agent\": \"query_agent\"}\n",
    "\n",
    "    def _preprocess_query(self, state: ChatState) -> ChatState:\n",
    "        \"\"\"Enhanced query preprocessing with more context-aware prompting.\"\"\"\n",
    "        query = state[\"query\"]\n",
    "        conversation_history = state.get(\"conversation_history\", [])\n",
    "\n",
    "        # Check cache first\n",
    "        cached_response = self.query_cache.get(query)\n",
    "        if cached_response:\n",
    "            return {\n",
    "                **state,\n",
    "                \"summary\": cached_response,\n",
    "                \"next_agent\": \"supervisor_agent\"\n",
    "            }\n",
    "\n",
    "        # specific prompt for Indian litigation context\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"You are a specialized legal assistant focusing on Indian litigation processes. \n",
    "            Your task is to formulate a comprehensive, legally precise search query that captures \n",
    "            the nuanced legal context of the user's question.\n",
    "\n",
    "            Consider the following guidelines:\n",
    "            - Extract key legal terminologies\n",
    "            - Identify specific areas of Indian litigation law\n",
    "            - Expand the query to include potential related legal concepts\n",
    "\n",
    "            Conversation History (if applicable):\n",
    "            {history}\n",
    "\n",
    "            Current Query: {query}\n",
    "\n",
    "            Enhanced Legal Search Query:\"\"\"\n",
    "        )\n",
    "\n",
    "        # Creating a chain for query enhancement\n",
    "        if conversation_history:\n",
    "            history_text = \"\\n\".join([f\"{msg['role'].upper()}: {msg['content']}\" for msg in conversation_history[-5:]])\n",
    "            \n",
    "            chain = (\n",
    "                {\"history\": lambda _: history_text, \"query\": lambda x: query}\n",
    "                | prompt\n",
    "                | self.llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            enhanced_query = chain.invoke(\"\")\n",
    "        else:\n",
    "            enhanced_query = query\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"enhanced_query\": enhanced_query,\n",
    "            \"next_agent\": \"query_agent\"\n",
    "        }\n",
    "\n",
    "    def _generate_response(self, state: ChatState) -> ChatState:\n",
    "        \"\"\"Generate response with more detailed, context-aware prompting.\"\"\"\n",
    "        summary = state[\"summary\"]\n",
    "        legal_references = state.get(\"legal_references\", [])\n",
    "        conversation_history = state.get(\"conversation_history\", [])\n",
    "        query = state[\"query\"]\n",
    "\n",
    "        # Create a prompt specifically tailored to Indian litigation context\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"As an expert in Indian litigation law, provide a comprehensive, \n",
    "            legally precise response that:\n",
    "            - Directly addresses the specific legal query\n",
    "            - Explains relevant legal principles and procedures\n",
    "            - References specific sections of Indian legal framework when applicable\n",
    "            - Uses clear, professional language accessible to non-legal professionals\n",
    "\n",
    "            Key Considerations for Indian Litigation:\n",
    "            - Align response with current Indian legal practices\n",
    "            - Highlight any procedural nuances specific to Indian courts\n",
    "            - Provide context about potential legal implications\n",
    "\n",
    "            Legal Summary: {summary}\n",
    "\n",
    "            Detailed Legal Response:\"\"\"\n",
    "        )\n",
    "\n",
    "        # Create a chain for response generation with streaming\n",
    "        chain = (\n",
    "            {\"summary\": lambda x: summary}\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        answer = chain.invoke({}) # Generate the answer\n",
    "        self.query_cache.set(query, answer) # Cache the response\n",
    "\n",
    "        # Update conversation history\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        if not conversation_history or conversation_history[-1][\"role\"] != \"human\":\n",
    "            conversation_history.append({\n",
    "                \"role\": \"human\",\n",
    "                \"content\": query,\n",
    "                \"timestamp\": current_time\n",
    "            })\n",
    "\n",
    "        conversation_history.append({\n",
    "            \"role\": \"ai\",\n",
    "            \"content\": answer,\n",
    "            \"timestamp\": current_time\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"answer\": answer,\n",
    "            \"conversation_history\": conversation_history,\n",
    "            \"next_agent\": END\n",
    "        }\n",
    "\n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build the LangGraph for the multi-agent system.\"\"\"\n",
    "        graph = StateGraph(ChatState)\n",
    "\n",
    "        # Add nodes with updated agents\n",
    "        graph.add_node(\"supervisor_agent\", self.supervisor_agent)\n",
    "        graph.add_node(\"query_agent\", self._query_agent)\n",
    "        graph.add_node(\"summarization_agent\", self._summarization_agent)\n",
    "\n",
    "        # Define conditional edges\n",
    "        graph.add_conditional_edges(\n",
    "            \"supervisor_agent\",\n",
    "            self._router,\n",
    "            {\n",
    "                \"query_agent\": \"query_agent\",\n",
    "                \"summarization_agent\": \"summarization_agent\",\n",
    "                END: END\n",
    "            }\n",
    "        )\n",
    "\n",
    "        graph.add_conditional_edges(\n",
    "            \"query_agent\",\n",
    "            self._router,\n",
    "            {\n",
    "                \"summarization_agent\": \"summarization_agent\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        graph.add_conditional_edges(\n",
    "            \"summarization_agent\",\n",
    "            self._router,\n",
    "            {\n",
    "                \"supervisor_agent\": \"supervisor_agent\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        graph.set_entry_point(\"supervisor_agent\")\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _router(self, state: ChatState) -> str:\n",
    "        \"\"\"Route to the next agent based on state.\"\"\"\n",
    "        return state.get(\"next_agent\", \"supervisor_agent\")\n",
    "\n",
    "    def _query_agent(self, state: ChatState) -> ChatState:\n",
    "        \"\"\"Query Agent that retrieves relevant information from legal documents.\"\"\"\n",
    "        query = state[\"enhanced_query\"]\n",
    "        docs = self.retriever.invoke(query)\n",
    "\n",
    "        context = [f\"Source: {doc.metadata.get('source', 'Unknown')}, Section: {doc.metadata.get('section', 'Unknown')}\\n\\n{doc.page_content}\" for doc in docs]\n",
    "        legal_references = [f\"{doc.metadata.get('source', 'Unknown')} - {doc.metadata.get('section', 'Unknown')}\" for doc in docs]\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"context\": context,\n",
    "            \"legal_references\": legal_references,\n",
    "            \"next_agent\": \"summarization_agent\"\n",
    "        }\n",
    "\n",
    "    def _summarization_agent(self, state: ChatState) -> ChatState:\n",
    "        \"\"\"Summarization Agent that simplifies legal information.\"\"\"\n",
    "        query = state[\"enhanced_query\"]\n",
    "        context = state[\"context\"]\n",
    "\n",
    "        if not context:\n",
    "            return {\n",
    "                **state,\n",
    "                \"summary\": \"I couldn't find specific legal information about your query in my knowledge base.\",\n",
    "                \"next_agent\": \"supervisor_agent\"\n",
    "            }\n",
    "\n",
    "        # More focused prompt for summarization\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"As a legal expert in Indian litigation, provide a precise, legally accurate summary \n",
    "            that addresses the user's specific legal question. Your summary should:\n",
    "            - Break down complex legal concepts\n",
    "            - Highlight key procedural aspects\n",
    "            - Provide clear, actionable insights\n",
    "            - Maintain professional legal terminology\n",
    "\n",
    "            User's Specific Legal Question: {query}\n",
    "\n",
    "            Relevant Legal Information:\n",
    "            {context}\n",
    "\n",
    "            Concise Legal Summary:\"\"\"\n",
    "        )\n",
    "\n",
    "        chain = (\n",
    "            {\"query\": RunnablePassthrough(), \"context\": lambda _: \"\\n\\n\".join(context)}\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        summary = chain.invoke(query)\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"summary\": summary,\n",
    "            \"next_agent\": \"supervisor_agent\"\n",
    "        }\n",
    "\n",
    "    def chat(self, query: str) -> str:\n",
    "        \"\"\"Process a user query and return a response.\"\"\"\n",
    "        state = {\n",
    "            \"query\": query,\n",
    "            \"enhanced_query\": \"\",\n",
    "            \"context\": [],\n",
    "            \"summary\": \"\",\n",
    "            \"answer\": \"\",\n",
    "            \"legal_references\": [],\n",
    "            \"conversation_history\": self.conversation_history,\n",
    "            \"next_agent\": \"supervisor_agent\"\n",
    "        }\n",
    "\n",
    "        result = self.graph.invoke(state)\n",
    "        self.conversation_history = result[\"conversation_history\"]\n",
    "\n",
    "        return result[\"answer\"]\n",
    "\n",
    "    def get_conversation_history(self) -> List[Message]:\n",
    "        \"\"\"Return the current conversation history.\"\"\"\n",
    "        return self.conversation_history\n",
    "\n",
    "    def clear_conversation(self) -> None:\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Legal Assistant: In response to your query regarding the validity of a contract governed by foreign law in India, it is important to note that Indian law recognizes the principle of party autonomy in contract law. This means that parties are generally free to choose the governing law of their contract, subject to certain conditions.\n",
      "\n",
      "The essential elements of a valid contract under Indian law include offer, acceptance, consideration, intention to create legal relations, capacity to contract, and lawful object and consideration. Therefore, if these elements are present in a contract governed by foreign law, it may be considered valid in India.\n",
      "\n",
      "However, it is crucial for parties to provide proof of the choice of foreign law in their contract. This can be done through a choice of law clause explicitly stating the governing law of the contract. In the absence of such proof, Indian law may default as the governing law of the contract.\n",
      "\n",
      "It is also important to consider the issue of jurisdiction in contracts governed by foreign law. While exclusive jurisdiction clauses are recognized in India, parties cannot confer jurisdiction on a court that lacks jurisdiction under Indian law. Indian courts may refuse to give up jurisdiction in cases involving foreign courts based on principles such as forum non conveniens, balance of convenience, and interests of justice.\n",
      "\n",
      "In recent years, efforts have been made to streamline the enforcement of contracts and judgments in India through amendments to the arbitration regime and the establishment of commercial courts. These developments aim to enhance the efficiency and effectiveness of dispute resolution mechanisms in India, including those involving contracts governed by foreign law.\n",
      "\n",
      "Overall, while parties have the freedom to choose the governing law of their contract, it is important to ensure compliance with Indian legal principles and procedures to avoid any potential legal implications. It is advisable to seek legal advice from a qualified Indian litigation lawyer to navigate the complexities of contract law in India effectively.\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = LegalChatbot()\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk me about Indian law (or type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        response = chatbot.chat(user_input)\n",
    "        print(\"\\nLegal Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
